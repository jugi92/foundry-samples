{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9290f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8748813b",
   "metadata": {},
   "source": [
    "# Mistral Document AI Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d704f0",
   "metadata": {},
   "source": [
    "In addition to basic OCR functionality, Mistral Document AI has annotations that allow you to extract information from documents and images in structured json with a single call to the API. It offers two types of annotations that can be used independently, or together:\n",
    "\n",
    "- bbox_annotation: gives you the annotation of the bounding boxes extracted by the OCR model (charts/figures etc.) based on a structure that you define in the request. This is provided for every image, in every page of the document.\n",
    "- document_annotation: like the bbox_annotation, but for the extracted text across the entire document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f024dee",
   "metadata": {},
   "source": [
    "> **Note**: Document annotations are currently limited at 8 pages. Please see the model card for the most up-to-date information on limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e0281",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0172af",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_MISTRAL_DOCUMENT_AI_ENDPOINT = \"https://YOUR-AI-FOUNDRY-NAME.services.ai.azure.com/providers/mistral/azure/ocr\"\n",
    "AZURE_MISTRAL_DOCUMENT_AI_KEY = \"\"\n",
    "\n",
    "REQUEST_HEADERS = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {AZURE_MISTRAL_DOCUMENT_AI_KEY}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72639aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder images\n",
    "import os\n",
    "\n",
    "os.makedirs(\"../images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f3d5ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../images/mistral7b.pdf', <http.client.HTTPMessage at 0x1edba81e150>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/mistral7b.pdf\", \"../images/mistral7b.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcff75f",
   "metadata": {},
   "source": [
    "## 1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "706550d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path: str) -> str:\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {image_path} was not found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5187390a",
   "metadata": {},
   "source": [
    "# 2. Bounding Box Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bbd992",
   "metadata": {},
   "source": [
    "In this example we will define a `bbox_annotation_format` as part of our request using JSON schema. In it, we will have fields for the type of image, and a short description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1329cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedDocument = encode_image(\"../images/mistral7b.pdf\")\n",
    "\n",
    "# check if doc was read\n",
    "assert encodedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3ef89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxannotationPayload = {\n",
    "    \"model\": \"mistral-document-ai-2505\",\n",
    "    \"document\": {\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": f\"data:application/pdf;base64,{encodedDocument}\",\n",
    "    },\n",
    "    \"include_image_base64\": \"true\",\n",
    "    \"bbox_annotation_format\": {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"string\",\n",
    "            \"description\": \"string\",\n",
    "            \"schema\": {\n",
    "                \"properties\": {\n",
    "                    \"image_type\": {\n",
    "                        \"description\": \"The type of the image.\",\n",
    "                        \"title\": \"Image Type\",\n",
    "                        \"type\": \"string\",\n",
    "                    },\n",
    "                    \"short_description\": {\n",
    "                        \"description\": \"A description in english describing the image.\",\n",
    "                        \"title\": \"Short Description\",\n",
    "                        \"type\": \"string\",\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "465e1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb1Response = requests.post(\n",
    "    url=AZURE_MISTRAL_DOCUMENT_AI_ENDPOINT,\n",
    "    json=bboxannotationPayload,\n",
    "    headers=REQUEST_HEADERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a1372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb1Response.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d74ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 0\n",
      "Image type: Logo\n",
      "Short description: A 3D-rendered logo of the text 'Mistral AI' in a gradient of warm colors, transitioning from orange to yellow.\n",
      "page 1\n",
      "Image type: diagram\n",
      "Short description: This image shows a comparison between Vanilla Attention and Sliding Window Attention mechanisms in the context of natural language processing. The left part of the image illustrates the attention patterns for the sentence 'The cat sat on the' using Vanilla Attention, where each word attends to all previous words. The middle part shows the attention patterns using Sliding Window Attention, where each word attends to a limited number of previous words within a sliding window. The right part of the image depicts the effective context length across different layers, indicating how the context length changes with the depth of the layers in the model. The diagram highlights the difference in how attention mechanisms handle the context of words in a sequence, with Vanilla Attention providing a broader context and Sliding Window Attention focusing on a more localized context.\n",
      "page 2\n",
      "Image type: Diagram\n",
      "Short description: A diagram showing the process of tokenization over three timesteps. Each row represents a different sentence being tokenized, with each word or part of a word represented in colored boxes. The colors change to indicate the progression of tokenization over time.\n",
      "page 2\n",
      "Image type: matrix\n",
      "Short description: This image shows a matrix used in natural language processing, specifically illustrating the presence of words in different contexts. The matrix is divided into three sections: Past, Cache, and Current. Each row represents a specific word (the, dog, go, to), and each column represents a word in the sentence 'The cat sat on the mat and saw the dog go to'. The numbers in the cells indicate the presence (1) or absence (0) of the row word in the corresponding position of the sentence. The Past section contains zeros, indicating no presence of the row words. The Cache section shows the presence of words as they transition, and the Current section shows the current presence of words in the sentence.\n",
      "page 3\n",
      "Image type: bar chart\n",
      "Short description: This bar chart compares the accuracy percentages of different models (Mistral 7B, LLaMA 2 13B, LLaMA 2 7B, and LLaMA 1 34B) across various categories. The left chart includes categories such as MMLU, Knowledge, Reasoning, and Comprehension, while the right chart includes AGI Eval, Math, BBH, and Code. Each category shows the performance of the models, with Mistral 7B generally performing the best in most categories, particularly in MMLU and AGI Eval. LLaMA 2 13B also shows strong performance, especially in Reasoning and Comprehension. LLaMA 2 7B and LLaMA 1 34B have more varied performances, with notable strengths in specific categories like Knowledge and Code respectively. The chart highlights the comparative strengths and weaknesses of each model across different tasks, providing insights into their relative effectiveness and areas of specialization.\n",
      "page 4\n",
      "Image type: chart\n",
      "Short description: The image contains four line charts comparing the performance of LLaMA 2 and Mistral models across different metrics (MMLU, Reasoning, Knowledge, and Comprehension) as a function of model size (in billions of parameters). Each chart shows the performance percentage on the y-axis and the model size on the x-axis. The charts indicate that LLaMA 2 consistently outperforms Mistral across all metrics and model sizes. The charts also highlight the effective sizes of LLaMA 2 models that match the performance of Mistral models, showing significant improvements in performance with larger model sizes for LLaMA 2.\n",
      "page 6\n",
      "Image type: 3D Render\n",
      "Short description: A 3D rendering of an orange letter 'M' with a small, cartoonish figure wearing boxing gloves standing on top of it.\n"
     ]
    }
   ],
   "source": [
    "for page in bb1Response.json()[\"pages\"]:\n",
    "    for image in page[\"images\"]:\n",
    "        print(\"page \" + str(page[\"index\"]))\n",
    "        iaj = json.loads(image[\"image_annotation\"])\n",
    "        print(\"Image type: \" + iaj[\"properties\"][\"image_type\"])\n",
    "        print(\"Short description: \" + iaj[\"properties\"][\"short_description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f647809",
   "metadata": {},
   "source": [
    "And here we see that we get the `image_annotation` in the response for each image in the document. It contains the fields we defined in the request for predictable extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa899a",
   "metadata": {},
   "source": [
    "# 3. Document Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39776ca0",
   "metadata": {},
   "source": [
    "Building off the bbox_annotations, we can add a `document_annotation_format` to our request. Just like the bbox_annotations, we define this in JSON schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46ef11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comboAnnotationPayload = {\n",
    "    \"model\": \"mistral-document-ai-2505\",\n",
    "    \"pages\": [0, 1, 2, 3],\n",
    "    \"document\": {\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": f\"data:application/pdf;base64,{encodedDocument}\",\n",
    "    },\n",
    "    \"include_image_base64\": \"true\",\n",
    "    \"bbox_annotation_format\": {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"string\",\n",
    "            \"description\": \"string\",\n",
    "            \"schema\": {\n",
    "                \"properties\": {\n",
    "                    \"image_type\": {\n",
    "                        \"description\": \"The type of the image.\",\n",
    "                        \"title\": \"Image Type\",\n",
    "                        \"type\": \"string\",\n",
    "                    },\n",
    "                    \"short_description\": {\n",
    "                        \"description\": \"A description in english describing the image.\",\n",
    "                        \"title\": \"Short Description\",\n",
    "                        \"type\": \"string\",\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"document_annotation_format\": {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"string\",\n",
    "            \"description\": \"string\",\n",
    "            \"schema\": {\n",
    "                \"properties\": {\n",
    "                    \"language\": {\n",
    "                        \"title\": \"Language\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The language of the document.\",\n",
    "                    },\n",
    "                    \"summary\": {\n",
    "                        \"title\": \"Summary\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"A brief summary of the document in English.\",\n",
    "                    },\n",
    "                    \"chapter_titles\": {\n",
    "                        \"title\": \"Chapter_Titles\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The titles of the chapters in the document.\",\n",
    "                    },\n",
    "                    \"urls\": {\n",
    "                        \"title\": \"urls\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The urls in the document.\",\n",
    "                    },\n",
    "                    \"translated_summary\": {\n",
    "                        \"title\": \"Translation\",\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The French translation of the document summary.\",\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36a2eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comboResponse = requests.post(\n",
    "    url=AZURE_MISTRAL_DOCUMENT_AI_ENDPOINT,\n",
    "    json=comboAnnotationPayload,\n",
    "    headers=REQUEST_HEADERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "775b1eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: English\n",
      "Summary: The document introduces Mistral 7B, a 7-billion-parameter language model designed for superior performance and efficiency. It outperforms other models like Llama 2 and Llama 1 in various benchmarks, including reasoning, mathematics, and code generation. The model uses grouped-query attention (GQA) and sliding window attention (SWA) for faster inference and reduced cost. Mistral 7B is released under the Apache 2.0 license and includes a fine-tuned instruction-following model, Mistral 7B-Instruct, which surpasses Llama 2 13B in human and automated benchmarks. The document also discusses the architectural details, including the use of sliding window attention and a rolling buffer cache to handle sequences of arbitrary length efficiently. Additionally, it presents results comparing Mistral 7B with Llama models across various tasks, showing Mistral 7B's superior performance. The document concludes with a discussion on instruction fine-tuning and the model's efficiency in terms of size and performance.\n",
      "Chapter Titles: Abstract, Introduction, Architectural details, Results, Instruction Finetuning\n",
      "URLs: https://github.com/mistralai/mistral-src, https://mistral.ai/news/announcing-mistral-7b/\n",
      "Translated Summary: Le document présente Mistral 7B, un modèle de langage de 7 milliards de paramètres conçu pour des performances et une efficacité supérieures. Il surpasse d'autres modèles comme Llama 2 et Llama 1 dans divers benchmarks, y compris le raisonnement, les mathématiques et la génération de code. Le modèle utilise l'attention par requête groupée (GQA) et l'attention par fenêtre coulissante (SWA) pour une inférence plus rapide et un coût réduit. Mistral 7B est publié sous la licence Apache 2.0 et comprend un modèle d'instruction affiné, Mistral 7B-Instruct, qui surpasse Llama 2 13B dans les benchmarks humains et automatisés. Le document discute également des détails architecturaux, y compris l'utilisation de l'attention par fenêtre coulissante et d'un cache tampon roulant pour traiter efficacement des séquences de longueur arbitraire. De plus, il présente des résultats comparant Mistral 7B avec les modèles Llama dans diverses tâches, montrant la performance supérieure de Mistral 7B. Le document se conclut par une discussion sur l'affinage des instructions et l'efficacité du modèle en termes de taille et de performance.\n"
     ]
    }
   ],
   "source": [
    "docAnnotation = json.loads(comboResponse.json()[\"document_annotation\"])\n",
    "print(\"Language: \" + docAnnotation[\"properties\"][\"language\"])\n",
    "print(\"Summary: \" + docAnnotation[\"properties\"][\"summary\"])\n",
    "print(\"Chapter Titles: \" + docAnnotation[\"properties\"][\"chapter_titles\"])\n",
    "print(\"URLs: \" + docAnnotation[\"properties\"][\"urls\"])\n",
    "print(\"Translated Summary: \" + docAnnotation[\"properties\"][\"translated_summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd80b9",
   "metadata": {},
   "source": [
    "And here we see the returned document annotations as we prescribed in the request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0b18e",
   "metadata": {},
   "source": [
    "# 4. Wrap-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e126d35",
   "metadata": {},
   "source": [
    "Being able to extract text and images from documents is powerful, when you combine this with structured extraction and enrichment it grants you the ability to create powerful document processing and intelligence capabilities. We hope you found this notebook useful, and look forward to seeing what you build with Mistral Document AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35186867",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
